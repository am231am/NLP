{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "uvm74-Iouoq3",
        "aBgb8V0vxe-F"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üåü [**Lemmatization**](https://www.analyticsvidhya.com/blog/2022/06/stemming-vs-lemmatization-in-nlp-must-know-differences/) üöÄ\n",
        "\n",
        "## üìå **1. What is Lemmatization?**\n",
        "Lemmatization is a process in **Natural Language Processing (NLP)** that finds the **base or dictionary form** of a word, called a **lemma**, by analyzing its **context** and **part of speech (POS)**.\n",
        "\n",
        "‚úÖ **Why is Lemmatization Important?**  \n",
        "- It ensures words are in their correct base form.  \n",
        "- It improves text analysis by maintaining proper meaning.  \n",
        "- It is widely used in **search engines, AI chatbots, and sentiment analysis**.\n",
        "\n",
        "---\n",
        "\n",
        "## üè∑ **2. What is Part of Speech (POS)?**\n",
        "**Part of Speech (POS)** refers to the **category** a word belongs to based on its grammatical function in a sentence.\n",
        "\n",
        "üîπ Words are classified into different parts of speech:\n",
        "\n",
        "| üè∑ **POS**      | üìñ **Definition**                | üîç **Example**            |\n",
        "|--------------|------------------------------|------------------------|\n",
        "| **Noun**     | A person, place, or thing.    | **dog, city, book**    |\n",
        "| **Verb**     | An action or state of being.  | **run, eat, is**       |\n",
        "| **Adjective**| Describes a noun.            | **big, happy, red**    |\n",
        "| **Adverb**   | Describes a verb or adjective. | **quickly, very, well** |\n",
        "| **Pronoun**  | Replaces a noun.              | **he, she, they**      |\n",
        "| **Preposition** | Shows relationships.      | **on, under, at**      |\n",
        "| **Conjunction** | Connects words/phrases.   | **and, but, because**  |\n",
        "| **Interjection** | Expresses emotion.       | **Wow!, Oh!, Oops!**   |\n",
        "\n",
        "---\n",
        "\n",
        "## üõ† **3. How Does Lemmatization Work?**\n",
        "Lemmatization follows a **structured** approach:\n",
        "\n",
        "1Ô∏è‚É£ **Tokenization** ‚Üí Break text into words.  \n",
        "2Ô∏è‚É£ **POS Tagging** ‚Üí Identify each word's role.  \n",
        "3Ô∏è‚É£ **Apply Lemmatization** ‚Üí Convert words to base form using a **linguistic database** (like WordNet).  \n",
        "4Ô∏è‚É£ **Output** ‚Üí Get meaningful and standardized words.  \n",
        "\n",
        "üìå **Example:**\n",
        "\n",
        "| üî§ **Word**  | üìù **Lemmatized Form** |\n",
        "|-------------|------------------|\n",
        "| **Running** | **Run** |\n",
        "| **Studies** | **Study** |\n",
        "| **Happily** | **Happy** |\n",
        "| **Better**  | **Good** |\n",
        "\n",
        "---\n",
        "\n",
        "# üéØ **4. Lemmatization vs. Stemming: Key Differences**\n",
        "Both techniques reduce words to their root forms, but **lemmatization is more precise!**  \n",
        "\n",
        "| ‚ö° **Feature**      | üîç **Lemmatization** | üîß **Stemming** |\n",
        "|----------------|-----------------|-------------|\n",
        "| **Definition**  | Converts words to their dictionary form, considering **context** and **POS**. | Removes **suffixes** without considering meaning. |\n",
        "| **Accuracy**    | ‚úÖ **High** ‚Äì produces valid words. | ‚ùå **Lower** ‚Äì may produce gibberish. |\n",
        "| **Speed**       | ‚ùå **Slower** (requires NLP rules). | ‚úÖ **Faster** (rule-based). |\n",
        "| **Use Case**    | **Chatbots, text understanding, AI.** | **Search engines, keyword matching.** |\n",
        "| **Example**     | \"better\" ‚Üí **\"good\"**, \"running\" ‚Üí **\"run\"** | \"better\" ‚Üí **\"bet\"**, \"running\" ‚Üí **\"run\"** |\n",
        "\n",
        "üìå **Comparison Example:**\n",
        "\n",
        "| üî§ **Word**  | üìù **Lemmatization** | üîß **Stemming** |\n",
        "|-------------|------------------|-------------|\n",
        "| **Running** | **Run** | **Run** |\n",
        "| **Studies** | **Study** | **Studi** |\n",
        "| **Happily** | **Happy** | **Happi** |\n",
        "| **Better**  | **Good**  | **Bet** |\n",
        "\n",
        "‚úÖ **Lemmatization = More accurate, preserves meaning.**  \n",
        "‚ùå **Stemming = Faster, but may distort words.**\n",
        "\n",
        "---\n",
        "\n",
        "# üßê **5. Should I Use Both Stemming and Lemmatization?**\n",
        "Follow these **5 simple steps** to decide which method fits your NLP task.  \n",
        "\n",
        "### **Step 1: Define Your Needs**\n",
        "üîπ **What is your goal?**  \n",
        "- Need to group **similar concepts** (e.g., synonyms)?  \n",
        "- Need to **preserve exact meaning** for **AI embeddings**?  \n",
        "\n",
        "üîπ **What matters more‚ÄîSpeed or Accuracy?**  \n",
        "- **Large dataset?** ‚Üí **Speed is crucial.**  \n",
        "- **Precise meaning?** ‚Üí **Accuracy is more important.**  \n",
        "\n",
        "---\n",
        "\n",
        "### **Step 2: Consider the Trade-Offs**\n",
        "| ‚ö° **Feature**      | üîß **Stemming** | üìù **Lemmatization** |\n",
        "|----------------|-------------|----------------|\n",
        "| **Speed**     | ‚úÖ **Fast** | ‚ùå **Slower** |\n",
        "| **Accuracy**  | ‚ùå May distort words | ‚úÖ Preserves meaning |\n",
        "| **Output**    | ‚ùå Can create non-words | ‚úÖ Produces real words |\n",
        "| **Best Use**  | **Search engines, large datasets** | **AI, chatbots, sentiment analysis** |\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 3: Choose Based on Your Needs**\n",
        "‚úÖ **Use Stemming if:**  \n",
        "- You need **fast processing**.  \n",
        "- You can tolerate **some loss of meaning**.  \n",
        "\n",
        "‚úÖ **Use Lemmatization if:**  \n",
        "- **Accuracy** is essential.  \n",
        "- You need grammatically correct base words.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Step 4: Experiment with Both**  \n",
        "If unsure, try both on a **small dataset** and compare:  \n",
        "- **Does stemming distort words too much?**  \n",
        "- **Does lemmatization slow down processing significantly?**  \n",
        "\n",
        "---\n",
        "\n",
        "### **Step 5: Refine Your Approach**  \n",
        "- **If speed is critical**, use **stemming**.  \n",
        "- **If accuracy is more important**, use **lemmatization**.  \n",
        "- **Hybrid approach?** Try **stemming first** for quick reduction, then **lemmatization** for refinement.  \n",
        "\n",
        "üí° **Final Tip:** The best choice depends on your specific **NLP task**! üöÄ  \n"
      ],
      "metadata": {
        "id": "9i7wFMO-esqF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**üìå Real-World Use Cases**\n",
        "## **1. Sentiment Analysis (Product Reviews, Social Media Monitoring)**\n",
        "\n",
        "### ‚úÖ **Best Choice: Lemmatization**  \n",
        "\n",
        "- **Why?** Lemmatization helps **normalize words** while maintaining their correct meaning.  \n",
        "- **Example:**  \n",
        "  - If a user writes: **\"This product is better than I expected!\"**  \n",
        "  - Lemmatization converts **\"better\" ‚Üí \"good\"**, helping the sentiment model recognize it as **positive sentiment**.  \n",
        "\n",
        "---\n",
        "\n",
        "### üí° **Example Sentiment Analysis:**\n",
        "\n",
        "| üìù **Raw Text**                     | üéØ **Lemmatized Text**         | üòÉ **Sentiment**  |\n",
        "|--------------------------------------|--------------------------------|------------------|\n",
        "| \"I loved the movies\"                 | \"I love the movie\"            | **Positive** üëç  |\n",
        "| \"This phone is worse than before\"    | \"This phone be bad than before\" | **Negative** üëé  |\n",
        "| \"Running is exhausting\"              | \"Run be exhaust\"              | **Neutral** üòê  |\n",
        "\n",
        "---\n",
        "\n",
        "### **Why Use Lemmatization for Sentiment Analysis?**\n",
        "‚úî **Maintains correct word meaning** (e.g., \"better\" ‚Üí \"good\")  \n",
        "‚úî **Reduces variations of words** for better text processing  \n",
        "‚úî **More accurate sentiment classification**  \n",
        "\n",
        "----\n",
        "\n",
        "# **2. üìÇ Search Engine for a Company‚Äôs Internal Reports**  \n",
        "\n",
        "**Scenario:** A company with thousands of internal reports (e.g., project updates, meeting notes) needs a search tool to help employees find documents quickly. Speed is critical because employees need results in seconds, and the database is large.\n",
        "\n",
        "## ‚úÖ **Best Choice: Stemming**  \n",
        "\n",
        "### **üöÄ Why Stemming?**  \n",
        "- **Speed is critical** ‚Äì Employees need results **instantly**.  \n",
        "- **Large database** ‚Äì Processing must be **fast and efficient**.  \n",
        "- **Employees use different word forms** ‚Äì Stemming helps match them quickly.  \n",
        "\n",
        "---\n",
        "\n",
        "## **üí° Example: Searching for \"meeting\"**\n",
        "| **Search Query**     | **Stemming Applied**  | **Matches Found**        |\n",
        "|----------------------|----------------------|--------------------------|\n",
        "| \"meeting notes\"     | \"meet\"               | ‚úÖ \"meeting notes\"       |\n",
        "| \"meetings summary\"  | \"meet\"               | ‚úÖ \"meetings summary\"    |\n",
        "| \"met with team\"     | \"met\" ‚Üí \"meet\"       | ‚úÖ \"met with team\"       |\n",
        "\n",
        "üìå **Stemming helps match related words, making searches broader and faster!**  \n",
        "\n",
        "---\n",
        "\n",
        "## **‚ùå Why Not Lemmatization?**  \n",
        "- **Slower processing** ‚Üí Lemmatization requires **linguistic analysis**, increasing search time.  \n",
        "- **Overkill for simple searches** ‚Üí Employees need **fast** keyword-based results, not deep semantic understanding.  \n",
        "\n",
        "---\n",
        "\n",
        "## **üîπ Final Decision: Use Stemming**  \n",
        "‚úî **Fast & efficient for large internal databases**  \n",
        "‚úî **Broadens search results by matching word variations**  \n",
        "‚úî **Ideal when speed matters more than perfect accuracy**  \n"
      ],
      "metadata": {
        "id": "xBekD4xzo4mG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Lemmatization using NLTK](https://gaurav5430.medium.com/using-nltk-for-lemmatizing-sentences-c1bfff963258)"
      ],
      "metadata": {
        "id": "uvm74-Iouoq3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVwK3M6GUCzx",
        "outputId": "c83a7d42-521d-4d57-a29a-781b091fa538"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZR73DQMGxnn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99bbf419-2dc3-44d2-c366-9750bdb38f30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n",
            "good\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Basic lemmatization\n",
        "print(lemmatizer.lemmatize(\"running\", pos=\"v\"))\n",
        "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Lemmatization using Spacy](https://spacy.io/api/lemmatizer)"
      ],
      "metadata": {
        "id": "DxZvvN_cv7Nn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load English NLP model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Process text\n",
        "doc = nlp(\"She is running better than her friend.\")\n",
        "\n",
        "# Print lemmatized words\n",
        "for token in doc:\n",
        "    print(token.text, \"‚Üí\", token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zBHcmujvciU",
        "outputId": "028e05c1-4819-4130-c62f-98ae11b2e3d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "She ‚Üí she\n",
            "is ‚Üí be\n",
            "running ‚Üí run\n",
            "better ‚Üí well\n",
            "than ‚Üí than\n",
            "her ‚Üí her\n",
            "friend ‚Üí friend\n",
            ". ‚Üí .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Remove contractions**\n",
        "# **üìå Expanding Contractions in NLP**  \n",
        "\n",
        "## **üöÄ What Are Contractions?**  \n",
        "**Contractions** are **shortened word forms** created by combining two words and **removing some letters**, replaced with an **apostrophe (`'`)**.  \n",
        "\n",
        "üîπ **Example:**  \n",
        "- `\"I'm\"` ‚Üí `\"I am\"`  \n",
        "- `\"Don't\"` ‚Üí `\"Do not\"`  \n",
        "- `\"It's\"` ‚Üí `\"It is\"`  \n",
        "\n",
        "---\n",
        "\n",
        "## **üìå Why Expand Contractions in NLP?** ü§ñ  \n",
        "Many **Natural Language Processing (NLP) models** struggle with contractions because they make text **informal and inconsistent**.  \n",
        "Expanding them helps in **better text preprocessing**, making it easier for AI models to analyze and understand sentences.  \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## **üìå Common Contractions & Their Expansions** üîÑ  \n",
        "\n",
        "| **Contraction** | **Expanded Form**  |  \n",
        "|---------------|----------------|  \n",
        "| I'm          | I am           |  \n",
        "| You're       | You are        |  \n",
        "| It's         | It is          |  \n",
        "| He's         | He is          |  \n",
        "| She's        | She is         |  \n",
        "| We're        | We are         |  \n",
        "| They're      | They are       |  \n",
        "| Isn't        | Is not         |  \n",
        "| Aren't       | Are not        |  \n",
        "| Can't        | Cannot        |  \n",
        "| Won't        | Will not       |  \n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "aBgb8V0vxe-F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install the Library**"
      ],
      "metadata": {
        "id": "2j9tgOANym_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install contractions"
      ],
      "metadata": {
        "id": "dsPonaIswD85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import contractions\n",
        "\n",
        "text = \"I'm learning NLP, but I won't give up!\"\n",
        "expanded_text = contractions.fix(text)\n",
        "\n",
        "print(f\"Before: {text}\")\n",
        "print(f\"After : {expanded_text}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vs30D698yy2m",
        "outputId": "290b71fd-43eb-4f86-9bfd-e98a6d224ed7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before: I'm learning NLP, but I won't give up!\n",
            "After : I am learning NLP, but I will not give up!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Remove Punctuation**"
      ],
      "metadata": {
        "id": "keuo7X1_weeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "#This is a constant in Python's string module that contains all standard punctuation characters.\n",
        "string.punctuation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2lf4l1dC2Hrp",
        "outputId": "cea774f0-6a7d-48cb-fc1d-d7dda4636e6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[string_maketrans](https://www.w3schools.com/python/ref_string_maketrans.asp)"
      ],
      "metadata": {
        "id": "9WBwZK9d4Bn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The maketrans() method returns a mapping table that can be used with the translate() method to replace specified characters.\n",
        "txt = \"Hello Sam!\"\n",
        "mytable = str.maketrans(\"S\", \"P\")\n",
        "print(mytable)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dns-vrO2YcV",
        "outputId": "6579da29-fe55-4652-dcde-ac092b125019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{83: 80}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a mapping table to replace many characters\n",
        "txt = \"Hi Sam!\"\n",
        "x = \"mSa\"\n",
        "y = \"eJo\"\n",
        "mytable = str.maketrans(x, y)\n",
        "print(txt.translate(mytable))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGAc6-Pk5mLs",
        "outputId": "205891cc-6630-4125-ee61-a73296d2b9df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi Joe!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The third parameter in the mapping table describes characters that you want to remove from the string\n",
        "txt = \"Good night Sam!\"\n",
        "x = \"mSa\"\n",
        "y = \"eJo\"\n",
        "z = \"odnght\"\n",
        "mytable = str.maketrans(x, y, z)\n",
        "print(txt.translate(mytable))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-Qe8YJ45tRq",
        "outputId": "c84acc04-97c4-4005-a7ad-29347bd764cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "G i Joe!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "text = \"Hello, World! How's everything?\"\n",
        "translator = str.maketrans('', '', string.punctuation)\n",
        "clean_text = text.translate(translator)\n",
        "\n",
        "print(f'Original: {text}')\n",
        "print(f'Cleaned : {clean_text}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krLxtK4n4Osx",
        "outputId": "f0281f47-1394-4f33-be81-6d7d43031595"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: Hello, World! How's everything?\n",
            "Cleaned : Hello World Hows everything\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W5tTWK_M5ljs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}